{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_A3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omF3m6NstGQ2"
      },
      "source": [
        "Submitted by Tarang Ranpara (202011057)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PUMbtrdQYG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb43203e-2400-4c5a-c95f-6d834dbc8a87"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('brown')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGrJPCVFZypv"
      },
      "source": [
        "def gen_dataset(data):\n",
        "    train_data, test_data =  train_test_split(data, test_size = 0.2, random_state = 50)\n",
        "\n",
        "    x_train = [[word.lower() for word,_ in sentence] for sentence in train_data]\n",
        "    x_test = [[word.lower() for word,_ in sentence] for sentence in test_data]\n",
        "\n",
        "    y_train = [[tag for _, tag in sentence] for sentence in train_data]\n",
        "    y_test = [[tag for _ , tag in sentence] for sentence in test_data]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_s3NTe8aq1N"
      },
      "source": [
        "class NLTK_POS_TAGGER:\n",
        "    def __init__(self, x_test, y_test):\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.y_pred = []\n",
        "\n",
        "    def predict(self):\n",
        "        for sentence in self.x_test:\n",
        "            pred = nltk.pos_tag(sentence, tagset='universal')\n",
        "            pred = [tag for _,tag in pred]\n",
        "            self.y_pred.append(pred)\n",
        "\n",
        "    def evaluate(self):\n",
        "        if self.y_pred == []:\n",
        "            self.predict()\n",
        "\n",
        "        total, match = 0, 0\n",
        "        for i in range(len(self.y_test)):\n",
        "            for pred, ground_truth in zip(self.y_pred[i], self.y_test[i]):\n",
        "                if pred == ground_truth:\n",
        "                    match += 1\n",
        "\n",
        "                total += 1\n",
        "\n",
        "        return match/total * 100"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHFyAiyRgNjt"
      },
      "source": [
        "class HiddenMarkovModel:\n",
        "    def __init__(self, x_train, y_train, x_test, y_test):\n",
        "        self.transition_matrix = dict()\n",
        "\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "        self.tags, self.words = set(), set() \n",
        "        self.tags, self.words = self.list_unique(self.y_train), self.list_unique(self.x_train)\n",
        "        self.y_pred = []\n",
        "\n",
        "    def fit(self):\n",
        "        print('Fitting the HMM model.')\n",
        "        self.start_probability = self.compute_start_probability()\n",
        "        self.transition_matrix = self.compute_transition_matrix()\n",
        "        self.emission_matrix = self.compute_emission_matrix()\n",
        "\n",
        "    def predict(self):\n",
        "        self.fit()\n",
        "        print('generating predictions.')\n",
        "        for sentence in self.x_test:\n",
        "            pred = self.viterbi_algo(sentence, self.tags, self.start_probability, self.transition_matrix, self.emission_matrix, self.words)\n",
        "            self.y_pred.append(pred)\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.predict()\n",
        "        print('evaulating the HMM model.')\n",
        "        total_words = 0\n",
        "        true_prediction = 0\n",
        "        total_sent = len(self.y_test)\n",
        "        for i in range(total_sent):\n",
        "            no_of_tokens = len(self.y_test[i])\n",
        "            for j in range(no_of_tokens):\n",
        "                if self.y_pred[i][j] == self.y_test[i][j]:\n",
        "                    true_prediction += 1\n",
        "                total_words += 1\n",
        "        acc = (true_prediction/total_words)*100\n",
        "        return acc\n",
        "\n",
        "    def list_unique(self, data):\n",
        "        unique_elements = set()\n",
        "        for sentence in data:\n",
        "            for token in sentence:\n",
        "                unique_elements.add(token)\n",
        "        return unique_elements\n",
        "\n",
        "    def compute_start_probability(self):\n",
        "        start_probability = dict()\n",
        "        \n",
        "        for tag in self.tags:\n",
        "            start_probability[tag] = 0\n",
        "        \n",
        "        for sentence in self.y_train:\n",
        "            start_probability[sentence[0]] += 1\n",
        "\n",
        "        sum_ = sum(start_probability.values())\n",
        "\n",
        "        for tag in self.tags:\n",
        "            start_probability[tag] /= sum_\n",
        "    \n",
        "        return start_probability\n",
        "\n",
        "    def compute_transition_matrix(self):\n",
        "        transition_matrix = dict()\n",
        "\n",
        "        for tag1 in self.tags:\n",
        "            transition_matrix[tag1] = dict() \n",
        "            for tag2 in self.tags:\n",
        "                transition_matrix[tag1][tag2] = 0\n",
        "        \n",
        "        for sentence in self.y_train:\n",
        "            m = len(sentence)\n",
        "            for i in range(1,m):\n",
        "                transition_matrix[sentence[i-1]][sentence[i]] += 1\n",
        "\n",
        "        for tag1 in self.tags:\n",
        "            sum_ = sum(transition_matrix[tag1].values())\n",
        "            for tag2 in self.tags:\n",
        "                transition_matrix[tag1][tag2] /= sum_\n",
        "        return transition_matrix\n",
        "\n",
        "    def compute_emission_matrix(self):\n",
        "        emission_matrix = dict()\n",
        "\n",
        "        for tag in self.tags:\n",
        "            emission_matrix[tag] = dict()\n",
        "            for word in self.words:\n",
        "                emission_matrix[tag][word] = 0\n",
        "\n",
        "        for sentence, tags in zip(self.x_train, self.y_train):\n",
        "            for tag, word in zip(tags, sentence):\n",
        "                emission_matrix[tag][word] +=  1 \n",
        "        \n",
        "        for tag in self.tags:\n",
        "            sum_ = sum(emission_matrix[tag].values())\n",
        "            for word in self.words:\n",
        "                emission_matrix[tag][word] /= sum_\n",
        "\n",
        "        return emission_matrix\n",
        "\n",
        "    def viterbi_algo(self, obs, states, s_pro, t_pro, e_pro, words):\n",
        "        path = { s:[s] for s in states}\n",
        "        curr_pro = {}\n",
        "        for s in states:\n",
        "            word_pro = 1 if obs[0] not in words else e_pro[s][obs[0]]\n",
        "            curr_pro[s] = s_pro[s]*word_pro\n",
        "\n",
        "        for i in range(1, len(obs)):\n",
        "            new_path = { s:[] for s in states}\n",
        "            last_pro = curr_pro.copy()\n",
        "            curr_pro = {}\n",
        "            for curr_state in states:\n",
        "                word_pro = 1 if obs[i] not in words else e_pro[curr_state][obs[i]]\n",
        "                max_pro, last_sta = max(((last_pro[last_state]*t_pro[last_state][curr_state]*word_pro, last_state) for last_state in states))\n",
        "                curr_pro[curr_state] = max_pro\n",
        "                new_path[curr_state] =path[last_sta] + [curr_state]\n",
        "            path = new_path\n",
        "        max_pro = -1\n",
        "        max_path = None\n",
        "        for s in states:\n",
        "            if curr_pro[s] > max_pro:\n",
        "                max_path = path[s]\n",
        "                max_pro = curr_pro[s]\n",
        "        return max_path"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EpTDl6T3uiM"
      },
      "source": [
        "tagged_corpus = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfcKmXPkaYCg"
      },
      "source": [
        "x_train, y_train, x_test, y_test = gen_dataset(tagged_corpus)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfVSh00Qaosx"
      },
      "source": [
        "nltk_pos_tagger = NLTK_POS_TAGGER(x_test, y_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBbDlsnXdDp2",
        "outputId": "2c2ae293-8f6d-4527-cbd5-1abe722d2fef"
      },
      "source": [
        "print(f'NLTK pos tagger accuracy: {nltk_pos_tagger.evaluate()}%')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK pos tagger accuracy: 90.71294639785317%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8EG7WCYeL4n"
      },
      "source": [
        "hmm = HiddenMarkovModel(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMBlkoZikbmo",
        "outputId": "4e8216ca-da11-411d-ffe5-a5cfaa19d4f3"
      },
      "source": [
        "print(f'HMM model accuracy: {hmm.evaluate()}%')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the HMM model.\n",
            "generating predictions.\n",
            "evaulating the HMM model.\n",
            "HMM model accuracy: 95.89468795155852%\n"
          ]
        }
      ]
    }
  ]
}