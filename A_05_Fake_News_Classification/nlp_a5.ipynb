{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-a5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3eTDxIXMWa3Q",
        "9ZyC8stlWliD",
        "_KZaqNSnWoim",
        "AeOYOZe6W_1F",
        "_13sgVmSW8c2",
        "MjibIcj-XEuf",
        "Hk3N1lSfXI3L"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_01AImdWTkF"
      },
      "source": [
        "submitted by Tarang Ranpara (202011057)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLGEe0JS5YqX"
      },
      "source": [
        "import os\n",
        "import spacy\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eTDxIXMWa3Q"
      },
      "source": [
        "# Exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzXPH8_i6r-c"
      },
      "source": [
        "train_path = 'drive/MyDrive/NLP_A5/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "7JdS3e0vQBn6",
        "outputId": "38f9ec6b-e8a0-4f0d-e2d2-4617a8f362ab"
      },
      "source": [
        "data = data.drop(data[~data.label.isin(['0', '1'])].index)\n",
        "print(\"Cleaned Dataset shape:\", data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Dataset shape: (20798, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  id  ... label\n",
              "0  0  ...     1\n",
              "1  1  ...     0\n",
              "2  2  ...     1\n",
              "3  3  ...     1\n",
              "4  4  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "0Fu6KsZM7Jnx",
        "outputId": "ec5ad624-1885-47b2-e4ef-806f7608a32a"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  id  ... label\n",
              "0  0  ...     1\n",
              "1  1  ...     0\n",
              "2  2  ...     1\n",
              "3  3  ...     1\n",
              "4  4  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF22P5QLNLDE"
      },
      "source": [
        "X = data[['id', 'title', 'author', 'text']]\n",
        "y = data[['label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcMb-c1N7VjD"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZyC8stlWliD"
      },
      "source": [
        "# Building the vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFcu3oMtQfQP"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {t: i for i, t in self.itos.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        logging.info(\"Building vocab\")\n",
        "        idx = 4\n",
        "        for sent in sentences:\n",
        "            for word in self.tokenizer_eng(sent):\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "        \n",
        "        logging.info(\"Vocab built.\")\n",
        "\n",
        "    def vectorize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "        \n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KZaqNSnWoim"
      },
      "source": [
        "# Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHvX9YL-SVXU"
      },
      "source": [
        "class DatasetLoader(Dataset):\n",
        "    def __init__(self, root_dir, subset=\"train\", transform=None, test_size=0.1):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.test_size = test_size\n",
        "        self.subset = subset\n",
        "\n",
        "        # Loading dataset \n",
        "        self.df = pd.read_csv(\n",
        "            os.path.join(root_dir, \"train.csv\"), \n",
        "            error_bad_lines=False, \n",
        "            warn_bad_lines=False, \n",
        "            engine=\"python\")\n",
        "        \n",
        "        # Cleaning dataset \n",
        "        self.__clean_data()\n",
        "\n",
        "        # Splitting the data \n",
        "        self.train_data, self.test_data = self.__train_test_split()\n",
        "\n",
        "        # Get texts and labels\n",
        "        self.texts = self.train_data[\"title\"].values\n",
        "        self.labels = self.train_data[\"label\"].values.astype(np.int64)\n",
        "\n",
        "        self.test_texts = self.test_data[\"title\"].values\n",
        "        self.test_labels = self.test_data[\"label\"].values.astype(np.int64)\n",
        "\n",
        "        self.classes = [\"Non Fake\", \"Fake\"]\n",
        "\n",
        "        # Initialize and build vocabulary\n",
        "        self.vocab = Vocab()\n",
        "        self.vocab.build_vocab(self.texts.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts) if self.subset == \"train\" else len(self.test_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.subset == \"train\":\n",
        "            text = self.texts[index]\n",
        "            label = self.labels[index]\n",
        "        else:\n",
        "            text = self.test_texts[index]\n",
        "            label = self.test_labels[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            text = self.transform(text)\n",
        "\n",
        "        vectorized_text = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        vectorized_text += self.vocab.vectorize(text)\n",
        "        vectorized_text.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        return vectorized_text, label\n",
        "    \n",
        "    def __clean_data(self):\n",
        "        self.df = self.df.dropna()\n",
        "        self.df = self.df.drop(self.df[~self.df.label.isin(['0', '1'])].index)\n",
        "    \n",
        "    def __train_test_split(self):\n",
        "        return train_test_split(self.df, test_size=self.test_size, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeOYOZe6W_1F"
      },
      "source": [
        "# pad the sequences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVTV-8v_TqFW"
      },
      "source": [
        "class PadTextSequence:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        texts = [torch.tensor(item[0]) for item in batch]\n",
        "        labels = [torch.tensor(item[1]) for item in batch]\n",
        "        texts = pad_sequence(texts, batch_first=True, padding_value=self.pad_idx)\n",
        "        \n",
        "        return texts, torch.Tensor(labels).to(torch.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_13sgVmSW8c2"
      },
      "source": [
        "# get train/test loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm05_uMgT1Q0"
      },
      "source": [
        "def get_train_test_loader(\n",
        "        root_fldr,\n",
        "        transform=None,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        test_split=0.1\n",
        "):\n",
        "    train_dataset = DatasetLoader(root_fldr, subset=\"train\", transform=transform, test_size=test_split)\n",
        "    test_dataset = DatasetLoader(root_fldr, subset=\"test\", transform=transform, test_size=test_split)\n",
        "    pad_idx = train_dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=PadTextSequence(pad_idx=pad_idx)\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=PadTextSequence(pad_idx=pad_idx)\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9s8Ueg9UYab"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjibIcj-XEuf"
      },
      "source": [
        "# setting hyper params "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciXV658FU3Db"
      },
      "source": [
        "# Set up hyperparameters\n",
        "num_layers = 1\n",
        "hidden_nodes = 256\n",
        "embedding_dim = 300\n",
        "learning_rate = 0.0001\n",
        "batch_size = 64\n",
        "num_epochs = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_vRTo8IUd6p",
        "outputId": "ae2ca7e2-6fb1-4fac-db7e-1d29b774b1a4"
      },
      "source": [
        "train_loader, test_loader = get_train_test_loader(train_path, batch_size=batch_size, shuffle=True)\n",
        "vocab_size = len(train_loader.dataset.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-15 16:39:37,732 : INFO : NumExpr defaulting to 2 threads.\n",
            "2021-09-15 16:39:37,755 : INFO : Building vocab\n",
            "2021-09-15 16:39:40,346 : INFO : Vocab built.\n",
            "2021-09-15 16:39:42,170 : INFO : Building vocab\n",
            "2021-09-15 16:39:44,115 : INFO : Vocab built.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk3N1lSfXI3L"
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiXShUzbVPx_"
      },
      "source": [
        "class LSTM_model(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        inp_size,\n",
        "        hidden_nodes=64, \n",
        "        num_layers=1,\n",
        "        embedding_dim=100\n",
        "    ):\n",
        "        super(LSTM_model, self).__init__()\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(inp_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_nodes, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_nodes, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h0 = torch.randn(self.num_layers, x.size(0), self.hidden_nodes).to(device)\n",
        "        c0 = torch.randn(self.num_layers, x.size(0), self.hidden_nodes).to(device)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x, [h0, c0])\n",
        "\n",
        "        # Consider only last hidden state\n",
        "        x = F.softmax(self.fc(x[:, -1, :]), dim=0)  \n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEO8kmt6W6dl"
      },
      "source": [
        "model = LSTM_model(vocab_size, hidden_nodes, num_layers, embedding_dim).to(device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcSveMs8XB2h"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PARcidxXFGt"
      },
      "source": [
        "def get_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # Switch model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():  # We don't need to compute gradients here\n",
        "        for x, y in tqdm(loader, ascii=\"123456789=\", desc=\"Evaluating:\"):\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            loss = criterion(scores, y)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)  # predictions.shape[0]\n",
        "\n",
        "        accuracy = (num_correct / num_samples) * 100\n",
        "        print(f\"Loss: {loss.item()}, Accuracy: {num_correct} / {num_samples} = {accuracy: .3f}%\")\n",
        "\n",
        "    # Switch back to training mode\n",
        "    model.train()\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxW9UVK7XMXJ"
      },
      "source": [
        "def train_loop():\n",
        "    print(\"Training begins..\")\n",
        "    for epoch in range(num_epochs):\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        loop = tqdm(enumerate(train_loader), total=len(train_loader), ascii=\" 123456789=\")\n",
        "        for batch_idx, (data, targets) in loop:\n",
        "            data = data.to(device=device)\n",
        "            # targets = targets.view(-1, 1).to(torch.float32)\n",
        "            targets = targets.to(device=device)\n",
        "\n",
        "            # Forward step\n",
        "            scores = model(data)\n",
        "            loss = criterion(scores, targets)\n",
        "\n",
        "            # Backward step\n",
        "            optimizer.zero_grad()  # To clear out previous step's gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient descent\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate ratio of correct predictions\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == targets).sum()\n",
        "            num_samples += predictions.size(0)  # predictions.shape[0]\n",
        "\n",
        "            # Update loss and accuracy on progress bar\n",
        "            accuracy = (num_correct / num_samples)\n",
        "            loop.set_description(f\"=> Epoch {epoch + 1}/{num_epochs}\")\n",
        "            loop.set_postfix(loss=loss.item(), accuracy=accuracy.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25RGMYzwXV9r",
        "outputId": "bb970024-935f-48ba-8d63-01128c8e1cf1"
      },
      "source": [
        "train_loop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training begins..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "=> Epoch 1/15: 100%|==========| 258/258 [04:35<00:00,  1.07s/it, accuracy=0.843, loss=0.557]\n",
            "=> Epoch 2/15: 100%|==========| 258/258 [04:30<00:00,  1.05s/it, accuracy=0.89, loss=0.61]\n",
            "=> Epoch 3/15: 100%|==========| 258/258 [04:30<00:00,  1.05s/it, accuracy=0.861, loss=0.562]\n",
            "=> Epoch 4/15: 100%|==========| 258/258 [04:33<00:00,  1.06s/it, accuracy=0.901, loss=0.545]\n",
            "=> Epoch 5/15: 100%|==========| 258/258 [04:31<00:00,  1.05s/it, accuracy=0.914, loss=0.556]\n",
            "=> Epoch 6/15: 100%|==========| 258/258 [04:26<00:00,  1.03s/it, accuracy=0.91, loss=0.55]\n",
            "=> Epoch 7/15: 100%|==========| 258/258 [04:26<00:00,  1.03s/it, accuracy=0.909, loss=0.564]\n",
            "=> Epoch 8/15: 100%|==========| 258/258 [04:23<00:00,  1.02s/it, accuracy=0.916, loss=0.557]\n",
            "=> Epoch 9/15: 100%|==========| 258/258 [04:23<00:00,  1.02s/it, accuracy=0.91, loss=0.546]\n",
            "=> Epoch 10/15: 100%|==========| 258/258 [04:22<00:00,  1.02s/it, accuracy=0.912, loss=0.555]\n",
            "=> Epoch 11/15: 100%|==========| 258/258 [04:22<00:00,  1.02s/it, accuracy=0.914, loss=0.549]\n",
            "=> Epoch 12/15: 100%|==========| 258/258 [04:25<00:00,  1.03s/it, accuracy=0.912, loss=0.556]\n",
            "=> Epoch 13/15: 100%|==========| 258/258 [04:30<00:00,  1.05s/it, accuracy=0.895, loss=0.558]\n",
            "=> Epoch 14/15: 100%|==========| 258/258 [04:29<00:00,  1.04s/it, accuracy=0.911, loss=0.552]\n",
            "=> Epoch 15/15: 100%|==========| 258/258 [04:28<00:00,  1.04s/it, accuracy=0.909, loss=0.563]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNct0dfsXZHM",
        "outputId": "56e54c95-c7de-4298-d95f-e1cf46463a39"
      },
      "source": [
        "test_accuracy = get_accuracy(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:: 100%|==========| 29/29 [00:01<00:00, 16.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6668935418128967, Accuracy: 1640 / 1829 =  89.666%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}